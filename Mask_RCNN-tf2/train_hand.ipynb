{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask R-CNN - Train on Shapes Dataset\n",
    "\n",
    "\n",
    "This notebook shows how to train Mask R-CNN on your own dataset. To keep things simple we use a synthetic dataset of shapes (squares, triangles, and circles) which enables fast training. You'd still need a GPU, though, because the network backbone is a Resnet101, which would be too slow to train on a CPU. On a GPU, you can start to get okay-ish results in a few minutes, and good results in less than an hour.\n",
    "\n",
    "The code of the *Shapes* dataset is included below. It generates images on the fly, so it doesn't require downloading any data. And it can generate images of any size, so we pick a small image size to train faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\C201_A~1\\AppData\\Local\\Temp/ipykernel_13272/1251644270.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandsConfig(Config):\n",
    "    \"\"\"Configuration for training on the toy shapes dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the toy shapes dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"HandsMasksOnly\"\n",
    "\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 16\n",
    "\n",
    "    MEAN_PIXEL = np.array([123.7, 116.8, 103.9, 103.9]) # for RGB-D\n",
    "    IMAGE_CHANNEL_COUNT = 4 \n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 1  # background + 3 classes\n",
    "\n",
    "    # Use small images for faster training. Set the limits of the small side\n",
    "    # the large side, and that determines the image shape.\n",
    "    IMAGE_MIN_DIM = 256 \n",
    "    IMAGE_MAX_DIM = 320\n",
    "    \n",
    "    USE_MINI_MASK = False\n",
    "    MINI_MASK_SHAPE = (64, 80)\n",
    "    MASK_SHAPE = [28, 28] # SHOULD BE CHANGED also somewhere else\n",
    "    IMAGE_RESIZE_MODE = \"deform\" # CUSTOM\n",
    "    \n",
    "    #BACKBONE = \"resnet50\"\n",
    "\n",
    "    # Use smaller anchors because our image and objects are small\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n",
    "\n",
    "    TRAIN_ROIS_PER_IMAGE = 10\n",
    "\n",
    "    # Use a small epoch since the data is simple\n",
    "    STEPS_PER_EPOCH = 100\n",
    "\n",
    "    # use small validation steps since the epoch is small\n",
    "    VALIDATION_STEPS = 5\n",
    "    \n",
    "config = HandsConfig()\n",
    "#config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Create a synthetic dataset\n",
    "\n",
    "Extend the Dataset class and add a method to load the shapes dataset, `load_shapes()`, and override the following methods:\n",
    "\n",
    "* load_image()\n",
    "* load_mask()\n",
    "* image_reference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import skimage.io\n",
    "class HandsDataset(utils.Dataset):\n",
    "\n",
    "    # overriden for RGB-D\n",
    "    def load_image(self, image_id):\n",
    "        image_depth = skimage.io.imread(self.image_info[image_id]['path'])\n",
    "        image_depth = image_depth[..., np.newaxis]\n",
    "        image_rgb = skimage.io.imread(self.image_info[image_id]['path'].replace(\"depth\",\"color\"))\n",
    "        # if image.ndim != 4: # TODO CHANGE WHEN INPUTS ARE GRAYSCALE/ 4channel\n",
    "            # image = np.stack((image[:,:,0],)*4, axis=-1)\n",
    "        image_full = np.concatenate([image_rgb, image_depth], axis=2)\n",
    "        return image_full\n",
    "\n",
    "    def load_dataset(self, dataset_dir, subset_suffix, config):\n",
    "        self.config = config\n",
    "        #self.add_class(\"hand\", 1, \"arm\")\n",
    "        self.add_class(\"hand\", 1, \"hand\")\n",
    "        #self.add_class(\"hand\", 3, \"fingertip\")\n",
    "\n",
    "        dataset_dir = os.path.join(dataset_dir, \"depth\"+subset_suffix)\n",
    "        image_ids = list(os.walk(dataset_dir))[0][2]\n",
    "\n",
    "        # Add images\n",
    "        for image_id in image_ids:\n",
    "            self.add_image(\n",
    "                \"hand\",\n",
    "                image_id=image_id,\n",
    "                path=os.path.join(dataset_dir, image_id),\n",
    "                subset = subset_suffix)\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for an image.\n",
    "       Returns:\n",
    "        masks: A bool array of shape [height, width, instance count] with\n",
    "            one mask per instance.\n",
    "        class_ids: a 1D array of class IDs of the instance masks.\n",
    "        \"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "\n",
    "        # mask_full = []\n",
    "        # for i in range(1,self.config.NUM_CLASSES):\n",
    "        #     mask_couter = str(i) if i > 0 else \"\"\n",
    "        #     mask_dir = os.path.join(os.path.dirname(os.path.dirname(info['path'])), \"mask\"+ mask_couter +info['subset'])\n",
    "        #     mask_file_name = info['id'].replace(\"depth\", \"mask\" + mask_couter)\n",
    "        #     mask = skimage.io.imread(os.path.join(mask_dir, mask_file_name)).astype(np.bool)[:,:,0] # TODO CHANGE WHEN MASKS ARE GRAYSCALE !!! \n",
    "        #     mask_full.append(mask)\n",
    "        # mask_full = np.stack(mask_full, axis=2)\n",
    "\n",
    "        # class_ids = np.array(list(range(1, self.config.NUM_CLASSES)))\n",
    "        #return mask_full, class_ids\n",
    "        \n",
    "        mask_dir = os.path.join(os.path.dirname(os.path.dirname(info['path'])), \"mask2\"+info['subset'])\n",
    "        mask = []\n",
    "        mask = skimage.io.imread(os.path.join(mask_dir, info['id'].replace(\"depth\", \"mask2\"))).astype(np.bool)\n",
    "        mask = mask[..., np.newaxis]\n",
    "        # Return mask, and array of class IDs of each instance. Since we have\n",
    "        # one class ID, we return an array of ones\n",
    "        return mask, np.ones([mask.shape[-1]], dtype=np.int32)\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the path of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"hand\":\n",
    "            return info[\"id\"]\n",
    "        else:\n",
    "            super(self.__class__, self).image_reference(image_id)\n",
    "            \n",
    "#DATASET_DIR = os.path.join(ROOT_DIR, \"datasets\", \"rgbd_joined_dataset\", \"ruka_2\")\n",
    "DATASET_DIR = os.path.join(\"D:/\", \"datasets\",\"blurred\",\"3800\")\n",
    "# Training dataset\n",
    "dataset_train = HandsDataset()\n",
    "dataset_train.load_dataset(DATASET_DIR,\"\", config)\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = HandsDataset()\n",
    "#dataset_val.load_dataset(DATASET_DIR,\"-valid\")\n",
    "dataset_val.load_dataset(DATASET_DIR,\"\", config)\n",
    "dataset_val.prepare()\n",
    "\n",
    "image_ids = np.random.choice(dataset_train.image_ids, 3)\n",
    "for image_id in image_ids:\n",
    "    image = dataset_train.load_image(image_id)\n",
    "    mask, class_ids = dataset_train.load_mask(image_id)\n",
    "    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names, limit = config.NUM_CLASSES - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create model in training mode\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config, model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Which weights to start with?\n",
    "init_with = \"coco\"  # imagenet, coco, or last, last_no_bbox\n",
    "\n",
    "# TODO copy weights for 4th channel - check https://github.com/orestis-z/mask-rcnn-rgbd/blob/d590e0f5085f8cbe895a6698e284426fd0116aa4/instance_segmentation/sceneNet/train.py#L66-L85\n",
    "\n",
    "if init_with == \"imagenet\":\n",
    "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
    "elif init_with == \"coco\":\n",
    "    # Load weights trained on MS COCO, but skip layers that\n",
    "    # are different due to the different number of classes\n",
    "    import copy\n",
    "    config_rgb = copy.deepcopy(config)\n",
    "    config_rgb.MEAN_PIXEL = config_rgb.MEAN_PIXEL[0:2]\n",
    "    config_rgb.IMAGE_SHAPE = np.array([config.IMAGE_MAX_DIM, config.IMAGE_MAX_DIM, 3])\n",
    "    model_rgb = modellib.MaskRCNN(mode=\"training\", config=config_rgb, model_dir=MODEL_DIR)\n",
    "    \n",
    "    # Local path to trained weights file\n",
    "    COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"Mask_RCNN-tf2/mask_rcnn_coco.h5\")\n",
    "    # Download COCO trained weights from Releases if needed\n",
    "    if not os.path.exists(COCO_MODEL_PATH):\n",
    "        utils.download_trained_weights(COCO_MODEL_PATH)\n",
    "    model_rgb.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "    conv1_rbg = model_rgb.keras_model.get_layer(\"conv1\")\n",
    "    kernel_rgb, bias_rgb = conv1_rbg.get_weights()\n",
    "    \n",
    "    model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\", \"conv1\"])\n",
    "    conv1_rgbd = model.keras_model.get_layer(\"conv1\")\n",
    "    kernel_rgbd = np.concatenate((kernel_rgb, np.mean(kernel_rgb, keepdims=True, axis=2)), axis=2)\n",
    "    conv1_rgbd.set_weights([kernel_rgbd, bias_rgb])\n",
    "\n",
    "elif init_with == \"last\":\n",
    "    # Load the last model you trained and continue training\n",
    "    model.load_weights(model.find_last(), by_name=True)\n",
    "elif init_with == \"last_no_bbox\":\n",
    "    # Load the last model you trained and continue training\n",
    "    model.load_weights(model.find_last(), by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\", \"conv1\"])\n",
    "elif init_with == \"last_no_top\":\n",
    "    # Load the last model you trained and continue training\n",
    "    model.load_weights(model.find_last(), by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Train in two stages:\n",
    "1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass `layers='heads'` to the `train()` function.\n",
    "\n",
    "2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass `layers=\"all` to train all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if init_with != \"last\":\n",
    "    # Train the head branches\n",
    "    # Passing layers=\"heads\" freezes all layers except the head\n",
    "    # layers. You can also pass a regular expression to select\n",
    "    # which layers to train by name pattern.\n",
    "    model.train(dataset_train, dataset_val, \n",
    "                learning_rate=config.LEARNING_RATE, \n",
    "                epochs=1, \n",
    "                layers='heads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine tune all layers\n",
    "# Passing layers=\"all\" trains all layers. You can also \n",
    "# pass a regular expression to select which layers to\n",
    "# train by name pattern.\n",
    "import imgaug\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=40, # THIS IS THE FINAL NUMBER OF EPOCHS, meaning if val =20 and we restart from 15 then only 5 epochs will be run !!!\n",
    "            layers=\"all\",\n",
    "            augmentation = imgaug.augmenters.Sequential([ \n",
    "                imgaug.augmenters.Fliplr(0.5), \n",
    "                imgaug.augmenters.Flipud(0.5)])\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save weights\n",
    "# Typically not needed because callbacks save after every epoch\n",
    "# Uncomment to save manually\n",
    "model_path = os.path.join(MODEL_DIR, \"mask_rcnn_hands_only.h5\")\n",
    "model.keras_model.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class InferenceConfig(HandsConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "    USE_MINI_MASK = False\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "model_path = os.path.join(MODEL_DIR, \"mask_rcnn_hands_only_orig.h5\")\n",
    "#model_path = model.find_last()\n",
    "\n",
    "# Load trained weights\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a random image from dataset\n",
    "# image_id = random.choice(dataset_val.image_ids)\n",
    "# original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "#     modellib.load_image_gt(dataset_val, inference_config, \n",
    "#                            image_id)\n",
    "\n",
    "# log(\"original_image\", original_image)\n",
    "# log(\"image_meta\", image_meta)\n",
    "# log(\"gt_class_id\", gt_class_id)\n",
    "# log(\"gt_bbox\", gt_bbox)\n",
    "# log(\"gt_mask\", gt_mask)\n",
    "\n",
    "# visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "#                             dataset_train.class_names, figsize=(8, 8))\n",
    "#                             results = model.detect([original_image], verbose=1)\n",
    "\n",
    "# r = results[0]\n",
    "# visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "#                             dataset_val.class_names, r['scores'], ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test on real image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mrcnn.utils import resize\n",
    "from importlib import reload\n",
    "reload(visualize)\n",
    "def evaluate_image(dataset_path, image_name, open_cv=True):\n",
    "    image_depth = skimage.io.imread(os.path.join(dataset_path, \"Depth\", image_name))\n",
    "    image_depth = resize(image_depth, (config.IMAGE_MIN_DIM, config.IMAGE_MAX_DIM), preserve_range=True)\n",
    "    image_depth = image_depth[..., np.newaxis]\n",
    "\n",
    "    image_rgb = skimage.io.imread(os.path.join(dataset_path, \"RGB\", image_name))\n",
    "    image_rgb = resize(image_rgb, (config.IMAGE_MIN_DIM, config.IMAGE_MAX_DIM), preserve_range=True)\n",
    "    image_full = np.concatenate([image_rgb, image_depth], axis=2)\n",
    "            \n",
    "    r = model.detect([image_full], verbose=0)[0]\n",
    "    visualize.display_instances(image_full, r['rois'], r['masks'], r['class_ids'], \n",
    "                        dataset_val.class_names, r['scores'],title=image_name, ax=get_ax(), min_score = 0.1)\n",
    "    \n",
    "test_images = [\"342\", \"341\", \"355\", \"374\", \"417\", \"452\", \"488\", \"503\", \"517\", \"525\"]\n",
    "real_imgs_path = os.path.join(\"C:/\", \"datasets\", \"camera_image\", \"2_ruce_rukavice_250\")\n",
    "\n",
    "\n",
    "for img in test_images:\n",
    "    evaluate_image(real_imgs_path, img+\".png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #x\n",
    "files = list(os.walk(os.path.join(real_imgs_path, \"Depth\")))[0][2]\n",
    "for f in files:\n",
    "    evaluate_image(real_imgs_path, f, open_cv=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for img in range(1,6):\n",
    "#     evaluate_image(real_imgs_path, str(img)+\".png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute VOC-Style mAP @ IoU=0.5\n",
    "# Running on 10 images. Increase for better accuracy.\n",
    "image_ids = np.random.choice(dataset_val.image_ids, 10)\n",
    "APs = []\n",
    "for image_id in image_ids:\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_val, inference_config,\n",
    "                               image_id)\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps =\\\n",
    "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "    APs.append(AP)\n",
    "    \n",
    "print(\"mAP: \", np.mean(APs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
